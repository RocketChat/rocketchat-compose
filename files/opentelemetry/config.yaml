extensions:
  docker_observer:
    endpoint: "unix:///var/run/docker.sock"

receivers:
  receiver_creator:
    watch_observers: [docker_observer]
    receivers:
      filelog:
        rule: type == "container"
        config:
          include:
            - /hostfs/storage/`container_id`/**/*.log
          include_file_path: true
          operators:
            # Route logs based on format (Docker JSON vs CRI)
            - type: router
              id: log_router
              routes:
                - output: parser_docker
                  expr: 'body matches "^{"'
                - output: parser_cri
                  expr: 'not(body matches "^{")'

            # Docker JSON Parser
            - type: json_parser
              id: parser_docker
              parse_from: body
              timestamp:
                parse_from: attributes.time
                layout_type: gotime
                layout: "2006-01-02T15:04:05.999999999Z07:00"
              output: extract_metadata

            # Manual CRI Parsing since 'container' operator fails on non-standard paths
            - type: regex_parser
              id: parser_cri
              regex: "^(?P<time>[^ ]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]+) ?(?P<log>.*)$"
              timestamp:
                parse_from: attributes.time
                layout_type: gotime
                layout: "2006-01-02T15:04:05.999999999Z07:00"

            # Recombine partial lines (logtag='P')
            - type: recombine
              combine_field: attributes.log
              is_last_entry: "attributes.logtag == 'F'"
              source_identifier: attributes["log.file.path"]

            - type: noop
              id: extract_metadata

            - type: move
              from: attributes.log
              to: body

            # Inject the tag from the container label
            - type: add
              field: attributes.tag
              value: '`labels["container_tag"]`'

            - type: add
              field: attributes.compose_container_name
              value: "`name`"

            # Extract our custom # driver: "json-file"project#service#name
            - id: regex_parser
              type: regex_parser
              parse_from: attributes.tag
              regex: "^(?P<compose_project>[^#]+)#(?P<compose_service>[^#]+)$"
              if: "attributes.tag != nil"

            # --- Metadata Mapping (The "Fake K8s" Logic) ---
            # Map captured regex groups to resource attributes
            - type: copy
              from: attributes.compose_container_name
              to: resource["k8s.container.name"]
              if: "attributes.compose_container_name != nil"

            - type: copy
              from: attributes.compose_service
              to: resource["k8s.deployment.name"]
              if: "attributes.compose_service != nil"

            # Also map service to pod name for consistency
            - type: copy
              from: attributes.compose_service
              to: resource["k8s.pod.name"]
              if: "attributes.compose_service != nil"

            - type: copy
              from: attributes.compose_project
              to: resource["k8s.namespace.name"]
              if: "attributes.compose_project != nil"

            # Clean up: the 'container' operator already moved log to body.
            # We just need to clean up temporary attributes if we want, but it's fine.

processors:
  resource:
    attributes:
      - key: job
        value: "logs"
        action: upsert
      - key: cluster.name
        value: "cross-engine-compose"
        action: upsert

exporters:
  otlphttp:
    endpoint: "http://loki:3100/otlp"

service:
  extensions: [docker_observer]
  pipelines:
    logs:
      receivers: [receiver_creator]
      processors: [resource]
      exporters: [otlphttp]
